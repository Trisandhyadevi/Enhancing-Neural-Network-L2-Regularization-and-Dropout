#  Improving Neural Networks with L2 and Dropout Regularization
## Introduction

Regularization techniques are essential for improving the generalization capabilities of neural networks and preventing overfitting. This repository includes implementations of two popular regularization methods: L2 regularization and dropout regularization. 

## L2 Regularization
L2 regularization, is a statistical technique used in machine learning to avoid overfitting. It involves adding a penalty term to the modelâ€™s loss function.

## Dropout 
Dropout regularization randomly drops a fraction of the neurons during training, which helps in preventing co-adaptation of neurons. This technique forces the network to learn redundant representations, making it more resilient to overfitting.

## Results
The results of applying L2 and dropout regularization show improved generalization and reduced overfitting. Detailed results and performance metrics can be found in the results directory.

## Acknowledgments
This work is part of a Coursera assignment. 
